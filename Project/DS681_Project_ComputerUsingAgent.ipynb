{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fmn19x_L-gun"
   },
   "outputs": [],
   "source": [
    "#Andrew Aquino\n",
    "#DS681 - Deep Learning for Computer Vision\n",
    "#Project Computer Using Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZPqYJOhCCGJ",
    "outputId": "e2562fbb-9ddf-4a51-f574-365acb456712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.12/dist-packages (4.15.5)\n",
      "\u001b[33mWARNING: pymongo 4.15.5 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo[srv]) (2.8.0)\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
      " Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "#This installs all the neccessary dependencies\n",
    "!pip install -q gradio transformers accelerate bitsandbytes sentencepiece protobuf\n",
    "!pip install -q pdf2image Pillow pymongo pydantic requests PyMuPDF\n",
    "!pip install -q torch torchvision opencv-python-headless numpy\n",
    "!pip install \"pymongo[srv]\"\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!apt-get install -q -y poppler-utils\n",
    "\n",
    "print(\" Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBGF0DKHCDKe",
    "outputId": "ffc5cd8a-164a-403b-ffc2-a4e2647321af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Llama model\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "#This Cell 2 starts the Ollama\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from threading import Thread\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def start_ollama():\n",
    "    subprocess.run([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "ollama_thread = Thread(target=start_ollama, daemon=True)\n",
    "ollama_thread.start()\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"Opening Llama model\")\n",
    "!ollama pull llama3.2:1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6xpo7yl-815",
    "outputId": "44aa9561-e6fa-49c7-83e1-7769c5995a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB Atlas.\n"
     ]
    }
   ],
   "source": [
    "#CELL 2.5 starts the MongoDB Connection using MongoDB Atlas\n",
    "from google.colab import userdata\n",
    "MONGO_URI = userdata.get('MONGO_URI')\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "DB = client[\"cua_agent\"]\n",
    "COLLECTION = DB[\"paper_responses\"]\n",
    "COLLECTION.create_index(\"session_id\")\n",
    "\n",
    "print(\"Successfully connected to MongoDB Atlas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zZ64V3AXCGAe"
   },
   "outputs": [],
   "source": [
    "#Cell 3: Import Libraries and Core Setup\n",
    "import gradio as gr\n",
    "import uuid\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse\n",
    "import gc\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import fitz\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pdf2image import convert_from_bytes\n",
    "import torch\n",
    "from transformers import AutoProcessor, Qwen3VLForConditionalGeneration, BitsAndBytesConfig\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Defining Global Variables for the VLM\n",
    "VLM_PROCESSOR = None\n",
    "VLM_MODEL = None\n",
    "CURRENT_SESSION = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CAoiH4wXCd5g"
   },
   "outputs": [],
   "source": [
    "#Cell 3.5: ways to organize the pdf files\n",
    "PAPER_STORAGE = {\n",
    "    \"current_paper\": { \"pages\": [], \"loaded\": False, \"filename\": None },\n",
    "    \"lock\": threading.Lock()\n",
    "}\n",
    "\n",
    "#storing the pdf paper being analyzed\n",
    "def store_paper(pages_data, filename):\n",
    "    with PAPER_STORAGE[\"lock\"]: PAPER_STORAGE[\"current_paper\"] = { \"pages\": pages_data, \"loaded\": True, \"filename\": filename }\n",
    "    logger.info(f\"Stored paper: {filename}, {len(pages_data)} pages\")\n",
    "\n",
    "#function to get the paper\n",
    "def get_paper():\n",
    "    with PAPER_STORAGE[\"lock\"]:\n",
    "        return PAPER_STORAGE[\"current_paper\"].copy()\n",
    "\n",
    "#to clear an saved paper in the cach\n",
    "def clear_paper():\n",
    "    with PAPER_STORAGE[\"lock\"]: PAPER_STORAGE[\"current_paper\"] = { \"pages\": [], \"loaded\": False, \"filename\": None }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Qx3yiJEWCR2c"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Models avaiable and intialize the Pydantic Agent\n",
    "\n",
    "# Available models\n",
    "AVAILABLE_VLM_MODELS = {\n",
    "    \"Qwen2-VL-7B (High Performance)\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"LLaVA 1.6 Mistral 7B (Fast)\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    \"BLIP-2 OPT 2.7B (Faster)\": \"Salesforce/blip2-opt-2.7b\",\n",
    "}\n",
    "\n",
    "AVAILABLE_LLM_MODELS = {\n",
    "    \"Llama 3.2 1B\": \"llama3.2:1b\",\n",
    "    \"Llama 3.2 3B\": \"llama3.2:3b\",\n",
    "}\n",
    "\n",
    "# Current configuration\n",
    "CONFIG = {\n",
    "    \"vlm_model\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    \"llm_model\": \"llama3.2:1b\",\n",
    "    \"max_pages\": 25,\n",
    "    \"dpi\": 150,\n",
    "    \"auto_analyze\": True\n",
    "}\n",
    "\n",
    "class PaperMetadata(BaseModel):\n",
    "    title: str\n",
    "    authors: List[str] = []\n",
    "    citation_count: int = 0\n",
    "    abstract: str = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6LfU9WHFCam5"
   },
   "outputs": [],
   "source": [
    "# Cell 5: These set of functions will be used to extract the highlighted text\n",
    "def detect_yellow_highlights(image_array):\n",
    "    hsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Yellow color range in HSV\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([35, 255, 255])\n",
    "\n",
    "    mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # create the bounding box\n",
    "    highlights = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w > 50 and h > 20:  # this will help filter the noise\n",
    "            highlights.append({\"x\": x, \"y\": y, \"width\": w, \"height\": h})\n",
    "\n",
    "    return highlights\n",
    "\n",
    "#this will get text from the highlighted region\n",
    "def extract_text_from_region(pdf_page, bbox):\n",
    "    x, y, w, h = bbox[\"x\"], bbox[\"y\"], bbox[\"width\"], bbox[\"height\"]\n",
    "\n",
    "    # PyMuPDF uses (x0, y0, x1, y1) format\n",
    "    rect = fitz.Rect(x, y, x + w, y + h)\n",
    "    text = pdf_page.get_text(\"text\", clip=rect)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "--HJ60s2Ci8n"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Loads the VLM after selecting the model\n",
    "\n",
    "def load_vlm_model(model_id):\n",
    "    global VLM_PROCESSOR, VLM_MODEL\n",
    "\n",
    "    logger.info(f\"Loading VLM: {model_id}\")\n",
    "\n",
    "    # Unload existing model\n",
    "    if VLM_MODEL is not None:\n",
    "        del VLM_MODEL, VLM_PROCESSOR\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True )\n",
    "\n",
    "    logger.info(\"Loading the selected VLM\")\n",
    "    VLM_PROCESSOR = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # This is for the Qwen2 model\n",
    "    if \"Qwen2-VL\" in model_id or \"Qwen/Qwen2\" in model_id:\n",
    "        from transformers import Qwen2VLForConditionalGeneration\n",
    "        VLM_MODEL = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        # this if for the other two models LLaVA and Blip\n",
    "        VLM_MODEL = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() #make sure I not using to much VRAM\n",
    "\n",
    "    logger.info(\"VLM loaded successfully\")\n",
    "    return True, f\"Loaded {model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lY25a7MKCmAM"
   },
   "outputs": [],
   "source": [
    "# Cell 7: These functions handle the pdf files\n",
    "\n",
    "def process_pdf_with_vlm(pdf_bytes, max_pages=25):\n",
    "    global VLM_PROCESSOR, VLM_MODEL\n",
    "\n",
    "    if VLM_MODEL is None:\n",
    "        return None, \"Please load a VLM model first\"\n",
    "\n",
    "    #just want to make sure to cap the dpi for shorter processing time\n",
    "    dpi = min(CONFIG[\"dpi\"], 200)\n",
    "    logger.info(f\"Converting PDF at {dpi} DPI\")\n",
    "\n",
    "    # this will convert PDF to images\n",
    "    images = convert_from_bytes(\n",
    "        pdf_bytes,\n",
    "        dpi=dpi,\n",
    "        fmt='png',\n",
    "        first_page=1,\n",
    "        last_page=max_pages\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Converted {len(images)} pages.\")\n",
    "    pages_data = []\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        page_num = i + 1\n",
    "        logger.info(f\"Processing page {page_num}/{len(images)}...\")\n",
    "\n",
    "        # Aggressive resizing to prevent memory issues\n",
    "        max_dim = 1024\n",
    "        original_size = img.size\n",
    "\n",
    "        if max(img.size) > max_dim:\n",
    "            ratio = max_dim / max(img.size)\n",
    "            new_size = tuple(int(dim * ratio) for dim in img.size)\n",
    "            img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            logger.info(f\"  Resized from {original_size} to {new_size}\")\n",
    "\n",
    "        # Detect yellow highlights\n",
    "        img_array = np.array(img)\n",
    "        highlights = detect_yellow_highlights(img_array)\n",
    "\n",
    "\n",
    "        # If using Qwen2-VL (different input format)\n",
    "        if \"Qwen2-VL\" in CONFIG[\"vlm_model\"]:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": img},\n",
    "                        {\"type\": \"text\", \"text\": \"Extract all text from this document page, maintaining structure. Include equations and tables.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            text = VLM_PROCESSOR.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = VLM_PROCESSOR(text=[text], images=[img], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        else:\n",
    "            # If using LLaVA/BLIP format\n",
    "            prompt = \"Extract all text from this document page, maintaining structure. Include equations and tables.\"\n",
    "            inputs = VLM_PROCESSOR(text=prompt, images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = VLM_MODEL.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        text = VLM_PROCESSOR.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean up output\n",
    "        if \"Extract all text\" in text:\n",
    "            text = text.split(\"Extract all text\")[-1].strip()\n",
    "\n",
    "        logger.info(f\" Extracted {len(text)} characters\")\n",
    "\n",
    "        # Store page data\n",
    "        pages_data.append({\n",
    "            \"page\": page_num,\n",
    "            \"text\": text,\n",
    "            \"highlights\": highlights,\n",
    "            \"has_yellow_highlights\": len(highlights) > 0\n",
    "        })\n",
    "\n",
    "        # making sure to cleanup after each page\n",
    "        del inputs, outputs, img, img_array\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(f\"Completed processing {len(pages_data)} pages\")\n",
    "\n",
    "    return pages_data, f\"Processed {len(pages_data)} pages\"\n",
    "\n",
    "#this will help identify important sections\n",
    "def analyze_paper_structure(pages_data):\n",
    "    important_sections = []\n",
    "\n",
    "    for page in pages_data:\n",
    "        text = page[\"text\"].lower()\n",
    "\n",
    "        # score given for important sections\n",
    "        importance_score = 0\n",
    "        reasons = []\n",
    "\n",
    "        if \"abstract\" in text:\n",
    "            importance_score += 10\n",
    "            reasons.append(\"Contains abstract\")\n",
    "\n",
    "        if \"contribution\" in text or \"novel\" in text:\n",
    "            importance_score += 8\n",
    "            reasons.append(\"Discusses contributions\")\n",
    "\n",
    "        if \"result\" in text or \"experiment\" in text:\n",
    "            importance_score += 7\n",
    "            reasons.append(\"Contains results/experiments\")\n",
    "\n",
    "        if \"table\" in text or \"figure\" in text:\n",
    "            importance_score += 5\n",
    "            reasons.append(\"References tables/figures\")\n",
    "\n",
    "        if \"relevance\" in text:\n",
    "            importance_score += 9\n",
    "            reasons.append(\"Relevant portions of the paper compared to other studies\")\n",
    "\n",
    "        if importance_score >= 7:\n",
    "            important_sections.append({\n",
    "                \"page\": page[\"page\"],\n",
    "                \"score\": importance_score,\n",
    "                \"reasons\": reasons,\n",
    "                \"text_preview\": page[\"text\"][:200] + \"...\"\n",
    "            })\n",
    "\n",
    "    return important_sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QIcaZ2ckCorE"
   },
   "outputs": [],
   "source": [
    "# Cell 8: The LLM logic for questions and answering\n",
    "\n",
    "#to make sure ollama is running correctly\n",
    "def check_ollama_health():\n",
    "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
    "    return r.status_code == 200\n",
    "\n",
    "\n",
    "def ensure_ollama_running():\n",
    "    if check_ollama_health():\n",
    "        return True\n",
    "\n",
    "    logger.warning(\"Ollama not responding, attempting restart\")\n",
    "\n",
    "\n",
    "    # Kill existing processes\n",
    "    os.system(\"pkill -9 ollama\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Restart\n",
    "    subprocess.Popen(\n",
    "        [\"nohup\", \"ollama\", \"serve\"],\n",
    "        stdout=open('/tmp/ollama.log', 'a'),\n",
    "        stderr=subprocess.STDOUT,\n",
    "        preexec_fn=os.setpgrp\n",
    "    )\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    if check_ollama_health():\n",
    "        logger.info(\"Ollama restarted successfully and is running\")\n",
    "        return True\n",
    "    else:\n",
    "        logger.error(\"Ollama restart failed\")\n",
    "        return False\n",
    "\n",
    "#this for the llm answer required questions regarding the pdf file\n",
    "def answer_question(question, pages_data, question_type=\"general\"):\n",
    "\n",
    "    combined_text = \"\\n\\n--- PAGE BREAK ---\\n\\n\".join( f\"[Page {p['page']}]\\n{p['text']}\" for p in pages_data )\n",
    "\n",
    "    # making customized prompts based on question type\n",
    "    #this case is for yellow highlights\n",
    "    if question_type == \"yellow_highlights\":\n",
    "        highlighted_pages = [p for p in pages_data if p[\"has_yellow_highlights\"]]\n",
    "        if not highlighted_pages:\n",
    "            return \"No yellow highlights detected in the document.\"\n",
    "\n",
    "        combined_text = \"\\n\\n\".join(\n",
    "            f\"[Page {p['page']} - Yellow Highlighted Text]\\n{p['text']}\"\n",
    "            for p in highlighted_pages\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\" You are a helpful tutor. The user has highlighted text in yellow.\n",
    "\n",
    "        Highlighted content from the paper: {combined_text}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Provide a clear, tutorial-style explanation of the highlighted content. Break down complex concepts step-by-step.\"\"\"\n",
    "\n",
    "    #this case is for the important sections question\n",
    "    elif question_type == \"important_sections\":\n",
    "        important = analyze_paper_structure(pages_data)\n",
    "        sections_text = \"\\n\\n\".join(\n",
    "            f\"[Page {s['page']}] (Importance: {s['score']}/10, Reasons: {', '.join(s['reasons'])})\\n{s['text_preview']}\"\n",
    "            for s in important\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"Identify and explain the most important sections in this paper.\n",
    "\n",
    "        Important sections detected: {sections_text}\n",
    "\n",
    "        Full paper text: {combined_text}\n",
    "\n",
    "        Explain why these sections are important and what key information they contain.\"\"\"\n",
    "\n",
    "    #this case is for the relevance of the paper\n",
    "    elif question_type == \"relevance\":\n",
    "        prompt = f\"\"\"Analyze the relevance and originality of the study in this paper.\n",
    "\n",
    "        Paper content: {combined_text}\n",
    "        Question: {question}\n",
    "        Focus on identifying relevance and originality of the study, how it relates to the world and its respective field of study.\"\"\"\n",
    "\n",
    "    else:\n",
    "        prompt = f\"\"\"You are a research analyst. Answer based on the paper content.\n",
    "\n",
    "        Paper text: {combined_text}\n",
    "        Question: {question}\n",
    "        Provide a detailed answer with page citations.\"\"\"\n",
    "\n",
    "    # just going to make sure to cut of the prompt if too long\n",
    "    if len(prompt) > 80000:\n",
    "        prompt = prompt[:80000] + \"\\n[...truncated...]\"\n",
    "\n",
    "    # Just to make Ollama is running before making request\n",
    "    if not ensure_ollama_running():\n",
    "        return \"Ollama service is not available. Please restart the notebook or run Cell 2 again.\"\n",
    "\n",
    "    # Call LLM with retry logic\n",
    "    max_retries = 2\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Sending request to Ollama (attempt {attempt + 1}/{max_retries})...\")\n",
    "\n",
    "            r = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json={\n",
    "                    \"model\": CONFIG[\"llm_model\"],\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0.7, \"num_predict\": 2000}\n",
    "                },\n",
    "                timeout=600  # 10 minute timeout\n",
    "            )\n",
    "\n",
    "            answer = r.json()[\"response\"]\n",
    "\n",
    "            # Store interaction for the MondoDB\n",
    "            COLLECTION.insert_one({\n",
    "                \"session_id\": CURRENT_SESSION,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"question\": question,\n",
    "                \"question_type\": question_type,\n",
    "                \"answer\": answer,\n",
    "                \"pages_analyzed\": len(pages_data)\n",
    "            })\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\" LLM Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9tYartah3Amu"
   },
   "outputs": [],
   "source": [
    "# Cell 9: This is the UI for the Gradio App\n",
    "\n",
    "#The gradio app function to define the User Interface\n",
    "def create_gradio_app():\n",
    "\n",
    "    with gr.Blocks(title=\"CUA Resarch Paper Analyzer \", theme=gr.themes.Soft()) as app:\n",
    "\n",
    "        gr.Markdown(\"# üìö CUA Resarch Paper Analyzer ü§ñ\")\n",
    "        gr.Markdown(\"Upload PDFs, detect highlights, and ask questions about academic papers\")\n",
    "\n",
    "        with gr.Tabs():\n",
    "\n",
    "            # This is for the first tab to choose the VLM, LLM and configuration setup\n",
    "            with gr.Tab(\"VLM and LLM Configuration ‚öôÔ∏è\"):\n",
    "                gr.Markdown(\"### Model Configuration\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    vlm_dropdown = gr.Dropdown(\n",
    "                        choices=list(AVAILABLE_VLM_MODELS.keys()),\n",
    "                        value=list(AVAILABLE_VLM_MODELS.keys())[0],\n",
    "                        label=\"Vision-Language Model (VLM)\"\n",
    "                    )\n",
    "\n",
    "                    llm_dropdown = gr.Dropdown(\n",
    "                        choices=list(AVAILABLE_LLM_MODELS.keys()),\n",
    "                        value=\"Llama 3.2 1B\",\n",
    "                        label=\"Large Language Model (LLM)\"\n",
    "                    )\n",
    "\n",
    "                load_vlm_btn = gr.Button(\"Load VLM Model\", variant=\"primary\")\n",
    "                vlm_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "                gr.Markdown(\"Processing Options\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    max_pages_slider = gr.Slider(\n",
    "                        minimum=1, maximum=30, value=15, step=1,\n",
    "                        label=\"Max Pages to Process\"\n",
    "                    )\n",
    "\n",
    "                    dpi_slider = gr.Slider(\n",
    "                        minimum=75, maximum=200, value=150, step=25,\n",
    "                        label=\"PDF DPI ( Higher DPI = Slower Proceessing Time )\"\n",
    "                    )\n",
    "\n",
    "\n",
    "                save_config_btn = gr.Button(\"Save Configuration\")\n",
    "                config_status = gr.Textbox(label=\"Config Status\", interactive=False)\n",
    "\n",
    "                def update_config(vlm_name, llm_name, max_pages, dpi):\n",
    "                    if dpi > 200:\n",
    "                        return \"DPI capped at 200 to prevent timeouts!\"\n",
    "\n",
    "                    CONFIG[\"vlm_model\"] = AVAILABLE_VLM_MODELS[vlm_name]\n",
    "                    CONFIG[\"llm_model\"] = AVAILABLE_LLM_MODELS[llm_name]\n",
    "                    CONFIG[\"max_pages\"] = max_pages\n",
    "                    CONFIG[\"dpi\"] = min(dpi, 200)\n",
    "                    return f\"Configuration Saved: {CONFIG['vlm_model']}, {CONFIG[\"llm_model\"]}\"\n",
    "\n",
    "                #this is for the buttons to load the VLM and LLM\n",
    "                load_vlm_btn.click(\n",
    "                    fn=lambda name: load_vlm_model(AVAILABLE_VLM_MODELS[name]),\n",
    "                    inputs=[vlm_dropdown],\n",
    "                    outputs=[vlm_status]\n",
    "                )\n",
    "\n",
    "                save_config_btn.click(\n",
    "                    fn=update_config,\n",
    "                    inputs=[vlm_dropdown, llm_dropdown, max_pages_slider, dpi_slider],\n",
    "                    outputs=[config_status]\n",
    "                )\n",
    "\n",
    "            # this is the second tab to upload the paper and check the status of the LLM and MongoDB\n",
    "            with gr.Tab(\"üìÑ PDF Paper Upload üîç\"):\n",
    "                gr.Markdown(\"### Upload PDF Document\")\n",
    "\n",
    "                pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "                process_btn = gr.Button(\"Process PDF\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    processing_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                    pages_processed = gr.Number(label=\"Pages Processed\", interactive=False)\n",
    "\n",
    "                paper_info = gr.JSON(label=\"Paper Structure Analysis\")\n",
    "\n",
    "                #this is the process the pdf file that was uploaded\n",
    "                def process_uploaded_pdf(pdf_file):\n",
    "                    if pdf_file is None:\n",
    "                        return \"No file uploaded\", 0, {}\n",
    "\n",
    "                    # read PDF bytes\n",
    "                    filename = pdf_file.name.split('/')[-1]\n",
    "                    with open(pdf_file.name, \"rb\") as f:\n",
    "                        pdf_bytes = f.read()\n",
    "\n",
    "                    # process with the VLM\n",
    "                    pages_data, status = process_pdf_with_vlm(pdf_bytes, CONFIG[\"max_pages\"])\n",
    "\n",
    "                    if pages_data is None:\n",
    "                        clear_paper()\n",
    "                        return status, 0, {}\n",
    "\n",
    "                    store_paper(pages_data, filename)\n",
    "                    important = analyze_paper_structure(pages_data)\n",
    "\n",
    "                    #this is what the vlm will output after analyzing the paper\n",
    "                    summary = {\n",
    "                        \"filename\": filename,\n",
    "                        \"total_pages\": len(pages_data),\n",
    "                        \"pages_with_highlights\": sum(1 for p in pages_data if p[\"has_yellow_highlights\"]),\n",
    "                        \"important_sections\": len(important),\n",
    "                        \"important_details\": important[:5]\n",
    "                    }\n",
    "\n",
    "                    return status, len(pages_data), summary\n",
    "\n",
    "                process_btn.click(\n",
    "                    fn=process_uploaded_pdf,\n",
    "                    inputs=[pdf_upload],\n",
    "                    outputs=[processing_status, pages_processed, paper_info]\n",
    "                )\n",
    "\n",
    "            #this is the third tab for the LLM questioning\n",
    "            with gr.Tab(\"Ask Questions Regarding the Paper\"):\n",
    "                gr.Markdown(\"### Ask Questions About the Paper\")\n",
    "\n",
    "\n",
    "                with gr.Row():\n",
    "                    check_services_btn = gr.Button(\"Ollama & MongoDB Status\", size=\"sm\")\n",
    "                    service_status = gr.Textbox(label=\"Current Status\", interactive=False, max_lines=2)\n",
    "                    paper_loaded_status = gr.Textbox(label=\"Paper Status\", interactive=False, max_lines=1)\n",
    "\n",
    "                # To check if Ollama and MongoDB are running\n",
    "                def check_services():\n",
    "                    ollama_ok = check_ollama_health()\n",
    "\n",
    "                    if client.server_info():\n",
    "                        mongo_ok = True\n",
    "                    else:\n",
    "                        mongo_ok = False\n",
    "\n",
    "                    status = []\n",
    "                    status.append(\"Ollama is running\" if ollama_ok else \"Ollama not running\")\n",
    "                    status.append(\"MongoDB is connected\" if mongo_ok else \"MongoDB disconnected\")\n",
    "\n",
    "                    # Check paper status\n",
    "                    paper = get_paper()\n",
    "                    if paper[\"loaded\"]:\n",
    "                        paper_status = f\"Paper loaded: {paper['filename']} ({len(paper['pages'])} pages)\"\n",
    "                    else:\n",
    "                        paper_status = \"Paper loaded unsuccessful\"\n",
    "\n",
    "                    return \" | \".join(status), paper_status\n",
    "\n",
    "                check_services_btn.click( fn=check_services, outputs=[service_status, paper_loaded_status] )\n",
    "\n",
    "                question_type_radio = gr.Radio(\n",
    "                    choices=[\n",
    "                        \"General Question\",\n",
    "                        \"Explain Yellow Highlighted Text\",\n",
    "                        \"Identify Important Sections\",\n",
    "                        \"What is the Papers Relevance?\"\n",
    "                    ],\n",
    "                    value=\"General Question\",\n",
    "                    label=\"Question Type\"\n",
    "                )\n",
    "\n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"Your Question\",\n",
    "                    placeholder=\"ex: What is the main contribution of this paper?\",\n",
    "                    lines=3\n",
    "                )\n",
    "\n",
    "                ask_btn = gr.Button(\"Get Answer\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "                answer_output = gr.Textbox(\n",
    "                    label=\"Answer\",\n",
    "                    lines=15,\n",
    "                    interactive=False\n",
    "                )\n",
    "\n",
    "                # Some predefined questions\n",
    "                gr.Markdown(\"### Quick Questions\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    q1_btn = gr.Button(\"Explain Yellow Highlighted Text\")\n",
    "                    q2_btn = gr.Button(\"Important Sections\")\n",
    "                    q3_btn = gr.Button(\"Papers Relevance\")\n",
    "\n",
    "                def handle_question(question, q_type):\n",
    "\n",
    "                    # Get paper from previously defined functions\n",
    "                    paper = get_paper()\n",
    "\n",
    "                    if not paper[\"loaded\"]:\n",
    "                        return \"Please upload and process a PDF first (Tab 2)\"\n",
    "\n",
    "                    pages_data = paper[\"pages\"]\n",
    "\n",
    "                    if not pages_data:\n",
    "                        return \"No pages found in the processed PDF\"\n",
    "\n",
    "                    type_map = {\n",
    "                        \"General Question\": \"general\",\n",
    "                        \"Explain Yellow Highlighted Text\": \"yellow_highlights\",\n",
    "                        \"Identify Important Sections\": \"important_sections\",\n",
    "                        \"Papers Relevance\": \"relevance\"\n",
    "                    }\n",
    "\n",
    "                    return answer_question(question, pages_data, type_map.get(q_type, \"general\"))\n",
    "\n",
    "                # more buttons for easy questions\n",
    "                ask_btn.click(\n",
    "                    fn=handle_question,\n",
    "                    inputs=[question_input, question_type_radio],\n",
    "                    outputs=[answer_output]\n",
    "                )\n",
    "\n",
    "                q1_btn.click(\n",
    "                    fn=lambda: handle_question(\"Explain the highlighted text\", \"Explain Yellow Highlights\"),\n",
    "                    outputs=[answer_output]\n",
    "                )\n",
    "\n",
    "                q2_btn.click(\n",
    "                    fn=lambda: handle_question(\"What are the important sections?\", \"Identify Important Sections\"),\n",
    "                    outputs=[answer_output]\n",
    "                )\n",
    "\n",
    "                q3_btn.click(\n",
    "                    fn=lambda: handle_question(\"What is the Papers Relevance?\", \"What is the Papers Relevance?\"),\n",
    "                    outputs=[answer_output]\n",
    "                )\n",
    "        return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "918706d5ec994374bb3629d87f945ef6",
      "20e7bab617874b4c9991462397ff67f5",
      "58bd7f17b7404d9c80f11b0f4fa8ea5c",
      "b910973ee7574dfd81b9f87590a3802b",
      "3acf72a0607047dfa5a4aad1370fb68e",
      "6f2df24ae5e64dbcbc2c7911075776c3",
      "ec0c354a082447989d72c16294150422",
      "b08993bc18c74c77929a6c239b0ccb75",
      "c46b6f4788304ab6b8e1f78d6e74bcf8",
      "bb9c7968dfa941f486171564a92746ea",
      "8ef121b6536a4ad0912e809ffc518a6a"
     ]
    },
    "id": "xQC4v_1-3FE-",
    "outputId": "bcbb97c9-a9f1-466d-d537-1919ad0be597"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1800443061.py:6: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
      "  with gr.Blocks(title=\"CUA Resarch Paper Analyzer \", theme=gr.themes.Soft()) as app:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Gradion App\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://275e4ca9faa9f3e026.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://275e4ca9faa9f3e026.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918706d5ec994374bb3629d87f945ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x7b0e31cb1dc0 [unset]> is bound to a different event loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://275e4ca9faa9f3e026.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10: This will Launch the Gradio App\n",
    "\n",
    "# Create and launch the app\n",
    "app = create_gradio_app()\n",
    "\n",
    "\n",
    "print(\"Launching Gradion App\")\n",
    "\n",
    "#thius will also create a url to be opened in the browser tab\n",
    "app.launch(\n",
    "    share=True,\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
