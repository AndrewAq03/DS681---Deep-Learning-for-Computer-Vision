{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "    !git clone https://github.com/facebookresearch/dinov3.git\n",
        "    %cd dinov3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3g4mR6Nr7Gz",
        "outputId": "58492153-1786-4bb9-ddb6-0382061a4e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dinov3'...\n",
            "remote: Enumerating objects: 503, done.\u001b[K\n",
            "remote: Counting objects: 100% (290/290), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 503 (delta 173), reused 103 (delta 93), pack-reused 213 (from 2)\u001b[K\n",
            "Receiving objects: 100% (503/503), 9.87 MiB | 18.45 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "/content/dinov3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install termcolor\n",
        "!pip install datasets huggingface_hub\n",
        "!pip install -q datasets fiftyone torch torchvision pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP_2BQo9r8hk",
        "outputId": "ad11c645-3bf4-40ba-f2a8-3a50020edf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import fiftyone as fo\n",
        "import torchvision.models as models\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "1aLeaOagrqMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=\"/content/drive/MyDrive/Assignment3/CarDD\",\n",
        "    dataset_type=fo.types.FiftyOneDataset\n",
        ")\n",
        "\n",
        "print(len(dataset), \"samples loaded\")\n",
        "print(dataset.first())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_-JIuX1AI4P",
        "outputId": "7db610a0-ad81-4a3d-b291-8ad169e1e5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.importers:Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 2816/2816 [114.9ms elapsed, 0s remaining, 25.0K samples/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 2816/2816 [114.9ms elapsed, 0s remaining, 25.0K samples/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2816 samples loaded\n",
            "<Sample: {\n",
            "    'id': '686be771e1d7135d782c77e9',\n",
            "    'media_type': 'image',\n",
            "    'filepath': '/content/drive/MyDrive/Assignment3/CarDD/data/000001.jpg',\n",
            "    'tags': [],\n",
            "    'metadata': <ImageMetadata: {\n",
            "        'size_bytes': None,\n",
            "        'mime_type': None,\n",
            "        'width': 1000,\n",
            "        'height': 750,\n",
            "        'num_channels': None,\n",
            "    }>,\n",
            "    'created_at': datetime.datetime(2025, 11, 2, 6, 6, 13, 129000),\n",
            "    'last_modified_at': datetime.datetime(2025, 11, 2, 6, 6, 13, 129000),\n",
            "    'detections': <Detections: {\n",
            "        'detections': [\n",
            "            <Detection: {\n",
            "                'id': '686be771e1d7135d782c77e5',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'scratch',\n",
            "                'bounding_box': [0.16704, 0.05361333333333333, 0.20279, 0.17512],\n",
            "                'mask': None,\n",
            "                'mask_path': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "                'iscrowd': 0,\n",
            "                'occluded': False,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '686be771e1d7135d782c77e6',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'tire flat',\n",
            "                'bounding_box': [\n",
            "                    0.16066,\n",
            "                    0.14993333333333334,\n",
            "                    0.6841900000000001,\n",
            "                    0.7346933333333333,\n",
            "                ],\n",
            "                'mask': None,\n",
            "                'mask_path': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "                'iscrowd': 0,\n",
            "                'occluded': False,\n",
            "            }>,\n",
            "        ],\n",
            "    }>,\n",
            "    'segmentations': <Detections: {\n",
            "        'detections': [\n",
            "            <Detection: {\n",
            "                'id': '686be771e1d7135d782c77e7',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'scratch',\n",
            "                'bounding_box': [0.16704, 0.05361333333333333, 0.20279, 0.17512],\n",
            "                'mask': array([[False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       ...,\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False]]),\n",
            "                'mask_path': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "                'iscrowd': 0,\n",
            "                'occluded': False,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '686be771e1d7135d782c77e8',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'tire flat',\n",
            "                'bounding_box': [\n",
            "                    0.16066,\n",
            "                    0.14993333333333334,\n",
            "                    0.6841900000000001,\n",
            "                    0.7346933333333333,\n",
            "                ],\n",
            "                'mask': array([[False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       ...,\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False],\n",
            "                       [False, False, False, ..., False, False, False]]),\n",
            "                'mask_path': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "                'iscrowd': 0,\n",
            "                'occluded': False,\n",
            "            }>,\n",
            "        ],\n",
            "    }>,\n",
            "    'coco_id': 1,\n",
            "}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPO_DIR = '/content/dinov3'\n",
        "weights = '/content/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth'\n",
        "dino =  torch.hub.load(REPO_DIR, 'dinov3_vitb16', source='local', weights=weights)"
      ],
      "metadata": {
        "id": "iLaXehlsAl8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this class takes the Fiftyone dataset and gets it ready to be used in the deep learning model\n",
        "class CarDD_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, image_size=(224, 224)): #left the default image size\n",
        "        self.samples = list(dataset)\n",
        "        self.image_size = image_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            #these values I used to normalized from large imagenet dataset\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = Image.open(sample.filepath).convert(\"RGB\") #just to make sure the image is RBG formate\n",
        "        mask = sample.segmentations.detections[0].mask #just going to get the scratch mask\n",
        "        # converts the numpy array mask to a PIL Image\n",
        "        mask_img = Image.fromarray(mask.astype(np.uint8) * 255, mode='L')\n",
        "        # then resize the image\n",
        "        mask_img = mask_img.resize(self.image_size, Image.NEAREST)\n",
        "\n",
        "        # make sure its a tensor and its normalized\n",
        "        mask_tensor = torch.from_numpy(np.array(mask_img)).float() / 255.0\n",
        "        mask_tensor = mask_tensor.unsqueeze(0)  #this adds a channel\n",
        "\n",
        "        return self.transform(image), mask_tensor\n",
        "\n",
        "\n",
        "class SegmentationHead(nn.Module):\n",
        "    #this is just a standard neural network bacbone\n",
        "    def __init__(self, in_channels=768, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "\n",
        "class DINOv3_Model(nn.Module):\n",
        "    #here we use the dino_pretrained model and the neural network class defined above\n",
        "    def __init__(self, backbone, head):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, x):\n",
        "        #the dino method that extracts features from a layer, this case I chose the last one\n",
        "        features = self.backbone.get_intermediate_layers(x, n=1)[0]\n",
        "        # get the dino outputs batch size, number of token and channel dimenstions\n",
        "        batch_size, number_tokens, channel_dimension = features.shape\n",
        "        height = width = int(number_tokens ** 0.5) #height and widght of patch gird\n",
        "        features = features.transpose(1, 2).reshape(batch_size, channel_dimension, height, width)\n",
        "        mask = self.head(features) #the decorder network\n",
        "        mask = F.interpolate(mask, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
        "        return mask\n",
        "\n",
        "\n",
        "# splitting the data into training and validation sets\n",
        "total_samples = len(dataset)\n",
        "train_size = int(0.8 * total_samples)\n",
        "val_size = total_samples - train_size\n",
        "\n",
        "train_dataset_fo = dataset.take(train_size)\n",
        "val_dataset_fo = dataset.skip(train_size)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset_fo)}\")\n",
        "print(f\"Validation samples: {len(val_dataset_fo)}\")\n",
        "\n",
        "# PyTorch datasets\n",
        "train_dataset = CarDD_Dataset(train_dataset_fo)\n",
        "val_dataset = CarDD_Dataset(val_dataset_fo)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "#the dino model\n",
        "model = DINOv3_Model(dino, SegmentationHead()).cuda()\n",
        "\n",
        "# this is so the backbone parameters is frozen\n",
        "print(\"Freezing backbone parameters...\")\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# using logits loss and standard Adam optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.head.parameters(), lr=1e-4)\n",
        "\n",
        "#num_epochs = 500\n",
        "num_epochs = 50 #had to use a way smaller epoch number\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# this function will get the bounding box from the segmentation mask\n",
        "def mask_to_bbox(mask):\n",
        "    mask_binary = (mask > 0.5).cpu().numpy().squeeze()\n",
        "    coords = np.argwhere(mask_binary)\n",
        "    if len(coords) == 0:\n",
        "        return None\n",
        "    y_min, x_min = coords.min(axis=0)\n",
        "    y_max, x_max = coords.max(axis=0)\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "# standard training loops\n",
        "print(\"\\nStarting training..\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.cuda(), masks.cuda()\n",
        "\n",
        "        # the forward pass\n",
        "        preds = model(images)\n",
        "        loss = criterion(preds, masks)\n",
        "\n",
        "        #backpopagration\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.cuda(), masks.cuda()\n",
        "            preds = model(images)\n",
        "            loss = criterion(preds, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # printing the progess every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # as the validation loss gets smaller, we save the best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, '/content/drive/MyDrive/Assignment3/best_segmentation_model.pth')\n",
        "        print(f\"   Best model saved! Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # also saving the model after every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, f'/content/drive/MyDrive/Assignment3/checkpoint_epoch_{epoch+1}.pth')\n",
        "        print(f\"   Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgMNhdzJJeiA",
        "outputId": "11802450-b736-4740-cde0-f9d795b927f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 2252\n",
            "Validation samples: 564\n",
            "Freezing backbone parameters...\n",
            "Trainable parameters: 1,934,337\n",
            "Total parameters: 87,603,969\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1551767456.py:21: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_img = Image.fromarray(mask.astype(np.uint8) * 255, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  → Best model saved! Val Loss: 0.5216\n",
            "  → Best model saved! Val Loss: 0.4843\n",
            "  → Best model saved! Val Loss: 0.4675\n",
            "  → Best model saved! Val Loss: 0.4588\n",
            "  → Best model saved! Val Loss: 0.4516\n",
            "  → Best model saved! Val Loss: 0.4457\n",
            "  → Best model saved! Val Loss: 0.4385\n",
            "  → Best model saved! Val Loss: 0.4295\n",
            "  → Best model saved! Val Loss: 0.4294\n",
            "Epoch 10/50 - Train Loss: 0.4310, Val Loss: 0.4212\n",
            "  → Best model saved! Val Loss: 0.4212\n",
            "  → Checkpoint saved at epoch 10\n",
            "  → Best model saved! Val Loss: 0.4186\n",
            "  → Best model saved! Val Loss: 0.4044\n",
            "  → Best model saved! Val Loss: 0.3984\n",
            "  → Best model saved! Val Loss: 0.3965\n",
            "  → Best model saved! Val Loss: 0.3849\n",
            "  → Best model saved! Val Loss: 0.3814\n",
            "  → Best model saved! Val Loss: 0.3736\n",
            "  → Best model saved! Val Loss: 0.3713\n",
            "Epoch 20/50 - Train Loss: 0.3420, Val Loss: 0.3549\n",
            "  → Best model saved! Val Loss: 0.3549\n",
            "  → Checkpoint saved at epoch 20\n",
            "  → Best model saved! Val Loss: 0.3541\n",
            "  → Best model saved! Val Loss: 0.3455\n",
            "  → Best model saved! Val Loss: 0.3446\n",
            "  → Best model saved! Val Loss: 0.3383\n",
            "  → Best model saved! Val Loss: 0.3380\n",
            "  → Best model saved! Val Loss: 0.3288\n",
            "  → Best model saved! Val Loss: 0.3227\n",
            "  → Best model saved! Val Loss: 0.3183\n",
            "Epoch 30/50 - Train Loss: 0.2723, Val Loss: 0.3190\n",
            "  → Checkpoint saved at epoch 30\n",
            "  → Best model saved! Val Loss: 0.3142\n",
            "  → Best model saved! Val Loss: 0.3124\n",
            "  → Best model saved! Val Loss: 0.2981\n",
            "Epoch 40/50 - Train Loss: 0.2278, Val Loss: 0.3024\n",
            "  → Checkpoint saved at epoch 40\n",
            "  → Best model saved! Val Loss: 0.2958\n",
            "  → Best model saved! Val Loss: 0.2938\n",
            "Epoch 50/50 - Train Loss: 0.1947, Val Loss: 0.2942\n",
            "  → Checkpoint saved at epoch 50\n",
            "\n",
            "Training completed!\n",
            "Best validation loss: 0.2938\n"
          ]
        }
      ]
    }
  ]
}