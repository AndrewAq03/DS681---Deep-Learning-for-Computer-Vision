{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Andrew Aquino\n",
        "#Hello this is my implementation of using the DeepSort Algorithm on a video I took of my car with dents using \n",
        "#the trained Dinov3 Segmenation Head PyTorch weights "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64EzmKKqZD8R",
        "outputId": "36d6f173-3df2-4a13-e0ff-d05791c9da76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting deep_sort_realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deep_sort_realtime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from deep_sort_realtime) (1.16.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from deep_sort_realtime) (4.12.0.88)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m255.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_sort_realtime\n",
            "Successfully installed deep_sort_realtime-1.3.2\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install deep_sort_realtime\n",
        "!pip install torchmetrics\n",
        "!pip install termcolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzGwCsXVaMXj",
        "outputId": "53c88f29-755b-48a6-cde4-a6a4defc8086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'dinov3'...\n",
            "remote: Enumerating objects: 503, done.\u001b[K\n",
            "remote: Counting objects: 100% (290/290), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 503 (delta 173), reused 103 (delta 93), pack-reused 213 (from 2)\u001b[K\n",
            "Receiving objects: 100% (503/503), 9.87 MiB | 18.86 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/dinov3.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xu-oBuCyaqid"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyhDmfNYZ_bd",
        "outputId": "719a0dd2-c801-4870-9873-924ac66c00a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"file:///content/drive/MyDrive/Assignment3/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\" to /root/.cache/torch/hub/checkpoints/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 327M/327M [00:08<00:00, 41.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#this is how the model is loaded in to be rain again\n",
        "repo = '/content/dinov3'\n",
        "weights = '/content/drive/MyDrive/Assignment3/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth'\n",
        "dino =  torch.hub.load(repo, 'dinov3_vitb16', source='local', weights=weights)\n",
        "\n",
        "class SegmentationHead(nn.Module):\n",
        "    #this is just a standard neural network bacbone\n",
        "    def __init__(self, in_channels=768, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "\n",
        "class DINOv3_Model(nn.Module):\n",
        "    #here we use the dino_pretrained model and the neural network class defined above\n",
        "    def __init__(self, backbone, head):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, x):\n",
        "        #the dino method that extracts features from a layer, this case I chose the last one\n",
        "        features = self.backbone.get_intermediate_layers(x, n=1)[0]\n",
        "        # get the dino outputs batch size, number of token and channel dimenstions\n",
        "        batch_size, number_tokens, channel_dimension = features.shape\n",
        "        height = width = int(number_tokens ** 0.5) #height and widght of patch gird\n",
        "        features = features.transpose(1, 2).reshape(batch_size, channel_dimension, height, width)\n",
        "        mask = self.head(features) #the decorder network\n",
        "        mask = F.interpolate(mask, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
        "        return mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjhiZ7HNY3fX",
        "outputId": "b80cd53c-611f-409a-93a8-638a4190c081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading trained model...\n",
            "Processing video: 1080x1920 @ 29fps, 2085 frames\n",
            "Starting processing...\n",
            "Processed 30/2085 frames, Detected 4 unique dents\n",
            "Processed 60/2085 frames, Detected 8 unique dents\n",
            "Processed 90/2085 frames, Detected 11 unique dents\n",
            "Processed 120/2085 frames, Detected 20 unique dents\n",
            "Processed 150/2085 frames, Detected 25 unique dents\n",
            "Processed 180/2085 frames, Detected 29 unique dents\n",
            "Processed 210/2085 frames, Detected 32 unique dents\n",
            "Processed 240/2085 frames, Detected 40 unique dents\n",
            "Processed 270/2085 frames, Detected 41 unique dents\n",
            "Processed 300/2085 frames, Detected 42 unique dents\n",
            "Processed 330/2085 frames, Detected 42 unique dents\n",
            "Processed 360/2085 frames, Detected 43 unique dents\n",
            "Processed 390/2085 frames, Detected 46 unique dents\n",
            "Processed 420/2085 frames, Detected 46 unique dents\n",
            "Processed 450/2085 frames, Detected 47 unique dents\n",
            "Processed 480/2085 frames, Detected 50 unique dents\n",
            "Processed 510/2085 frames, Detected 51 unique dents\n",
            "Processed 540/2085 frames, Detected 68 unique dents\n",
            "Processed 570/2085 frames, Detected 71 unique dents\n",
            "Processed 600/2085 frames, Detected 72 unique dents\n",
            "Processed 630/2085 frames, Detected 72 unique dents\n",
            "Processed 660/2085 frames, Detected 73 unique dents\n",
            "Processed 690/2085 frames, Detected 77 unique dents\n",
            "Processed 720/2085 frames, Detected 83 unique dents\n",
            "Processed 750/2085 frames, Detected 86 unique dents\n",
            "Processed 780/2085 frames, Detected 89 unique dents\n",
            "Processed 810/2085 frames, Detected 90 unique dents\n",
            "Processed 840/2085 frames, Detected 93 unique dents\n",
            "Processed 870/2085 frames, Detected 93 unique dents\n",
            "Processed 900/2085 frames, Detected 95 unique dents\n",
            "Processed 930/2085 frames, Detected 95 unique dents\n",
            "Processed 960/2085 frames, Detected 102 unique dents\n",
            "Processed 990/2085 frames, Detected 108 unique dents\n",
            "Processed 1020/2085 frames, Detected 116 unique dents\n",
            "Processed 1050/2085 frames, Detected 120 unique dents\n",
            "Processed 1080/2085 frames, Detected 127 unique dents\n",
            "Processed 1110/2085 frames, Detected 128 unique dents\n",
            "Processed 1140/2085 frames, Detected 129 unique dents\n",
            "Processed 1170/2085 frames, Detected 138 unique dents\n",
            "Processed 1200/2085 frames, Detected 138 unique dents\n",
            "Processed 1230/2085 frames, Detected 139 unique dents\n",
            "Processed 1260/2085 frames, Detected 139 unique dents\n",
            "Processed 1290/2085 frames, Detected 139 unique dents\n",
            "Processed 1320/2085 frames, Detected 141 unique dents\n",
            "Processed 1350/2085 frames, Detected 143 unique dents\n",
            "Processed 1380/2085 frames, Detected 143 unique dents\n",
            "Processed 1410/2085 frames, Detected 144 unique dents\n",
            "Processed 1440/2085 frames, Detected 146 unique dents\n",
            "Processed 1470/2085 frames, Detected 148 unique dents\n",
            "Processed 1500/2085 frames, Detected 148 unique dents\n",
            "Processed 1530/2085 frames, Detected 148 unique dents\n",
            "Processed 1560/2085 frames, Detected 148 unique dents\n",
            "Processed 1590/2085 frames, Detected 148 unique dents\n",
            "Processed 1620/2085 frames, Detected 148 unique dents\n",
            "Processed 1650/2085 frames, Detected 153 unique dents\n",
            "Processed 1680/2085 frames, Detected 154 unique dents\n",
            "Processed 1710/2085 frames, Detected 156 unique dents\n",
            "Processed 1740/2085 frames, Detected 162 unique dents\n",
            "Processed 1770/2085 frames, Detected 173 unique dents\n",
            "Processed 1800/2085 frames, Detected 175 unique dents\n",
            "Processed 1830/2085 frames, Detected 176 unique dents\n",
            "Processed 1860/2085 frames, Detected 179 unique dents\n",
            "Processed 1890/2085 frames, Detected 182 unique dents\n",
            "Processed 1920/2085 frames, Detected 184 unique dents\n",
            "Processed 1950/2085 frames, Detected 185 unique dents\n",
            "Processed 1980/2085 frames, Detected 186 unique dents\n",
            "Processed 2010/2085 frames, Detected 187 unique dents\n",
            "Processed 2040/2085 frames, Detected 188 unique dents\n",
            "Processed 2070/2085 frames, Detected 188 unique dents\n",
            "\n",
            "Processing complete!\n",
            "Total unique dents detected: 188\n",
            "Output saved to: /content/drive/MyDrive/Assignment3/output_tracked_dents_2.mp4\n",
            "\n",
            "✓ Done! Found 188 unique dents in the video.\n"
          ]
        }
      ],
      "source": [
        "# this function will load the model that I trained previously\n",
        "def load_trained_model(checkpoint_path, device='cuda'):\n",
        "    model = DINOv3_Model(dino, SegmentationHead()).to(device)\n",
        "\n",
        "    # loads the Pytoch model with the weights and evaluates\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "#this transformation is the same as the training to process the frames in the video\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def mask_to_bboxes(mask, confidence_threshold=0.5, min_area=100):\n",
        "\n",
        "    #htis is used for convert the mask probabilty to a binary image\n",
        "    mask_binary = (mask > confidence_threshold).astype(np.uint8) * 255\n",
        "\n",
        "    # using OpenCVs method we can get data from each compents (like bounding boxe)\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
        "        mask_binary, connectivity=8\n",
        "    )\n",
        "\n",
        "    bboxes = []\n",
        "\n",
        "    for i in range(1, num_labels):\n",
        "        x, y, w, h, area = stats[i]\n",
        "\n",
        "        # the  min area helps filter our unessary noise\n",
        "        if area < min_area:\n",
        "            continue\n",
        "\n",
        "        # just gives how good the bounding box is\n",
        "        component_mask = (labels == i)\n",
        "        avg_confidence = mask[component_mask].mean()\n",
        "\n",
        "        # just formatting the bounding boxes\n",
        "        bbox = (x, y, x + w, y + h, float(avg_confidence))\n",
        "        bboxes.append(bbox)\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "def process_frame(frame, model, device='cuda', orig_size=None):\n",
        "\n",
        "    # OpenCv likes to use BGR frame so this needs to be converteded to RBG\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "\n",
        "    input_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "    # the prediction portion which uses sigmoid for probabilty\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        mask = torch.sigmoid(output).squeeze().cpu().numpy()\n",
        "\n",
        "    # resizing the image back into the original\n",
        "    if orig_size is not None:\n",
        "        mask = cv2.resize(mask, (orig_size[1], orig_size[0]),\n",
        "                         interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "   # only take the bounding boxes with high confidence\n",
        "    bboxes = mask_to_bboxes(mask, confidence_threshold=0.5, min_area=100)\n",
        "\n",
        "    return mask, bboxes\n",
        "\n",
        "#this will help draw the dents\n",
        "def draw_dents(frame, tracks, mask=None, show_mask=True):\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # I wanted is to visually show the segmenations on the dent\n",
        "    # it should be like a semi-red almost transparent mask\n",
        "    if show_mask and mask is not None:\n",
        "        colored_mask = np.zeros_like(frame)\n",
        "        colored_mask[:, :, 2] = (mask * 255).astype(np.uint8)\n",
        "\n",
        "        annotated_frame = cv2.addWeighted(annotated_frame, 0.7, colored_mask, 0.3, 0)\n",
        "\n",
        "    for track in tracks:\n",
        "        if not track.is_confirmed():\n",
        "            continue\n",
        "\n",
        "        track_id = track.track_id\n",
        "        ltrb = track.to_ltrb()  # this is how to get the bounding boxes\n",
        "        x1, y1, x2, y2 = map(int, ltrb)\n",
        "\n",
        "        #get color for each bounding box and create them\n",
        "        color = get_color_for_id(track_id) #this function is below\n",
        "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
        "        label = f\"Dent #{track_id}\"\n",
        "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "\n",
        "        # The text that will be showing in the background of the video\n",
        "        cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), color, -1)\n",
        "        cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "    return annotated_frame\n",
        "\n",
        "#this function just generates a random color for each unique ID\n",
        "def get_color_for_id(track_id):\n",
        "    np.random.seed(int(track_id))\n",
        "    color = tuple(map(int, np.random.randint(0, 255, 3)))\n",
        "    return color\n",
        "\n",
        "#this is the main function that is needed for the video processing of the dents\n",
        "def video_processing(video_path, model, output_path='output_dents.mp4', device='cuda'):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    # Just preprocessiung stuff to get the video info\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"Processing video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
        "\n",
        "    # Initialize video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # This is the DeepSort Tracker\n",
        "    tracker = DeepSort(\n",
        "        max_age=30,          # Frames to keep track alive without detections\n",
        "        n_init=3,            # Frames needed to confirm a track\n",
        "        max_iou_distance=0.7, # IOU threshold for matching\n",
        "        embedder=\"mobilenet\", # Feature extractor for appearance\n",
        "        half=True,           # Use FP16 for speed\n",
        "        embedder_gpu=True    # Use GPU for embedder\n",
        "    )\n",
        "\n",
        "    unique_dent_ids = set()\n",
        "    frame_count = 0\n",
        "    print(\"Starting processing...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # while the video is being process for each frame try to get detections\n",
        "        mask, bboxes = process_frame(frame, model, device, orig_size=(height, width))\n",
        "\n",
        "        # need to convert the bounding boxes to DeepSort Format\n",
        "        detections = []\n",
        "        for bbox in bboxes:\n",
        "            x1, y1, x2, y2, confidence = bbox\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            detections.append(([x1, y1, w, h], confidence, 'dent'))\n",
        "\n",
        "        #this updates the tracked detections\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "        for track in tracks:\n",
        "            if track.is_confirmed():\n",
        "                unique_dent_ids.add(track.track_id)\n",
        "\n",
        "        annotated_frame = draw_dents(frame, tracks, mask, show_mask=True)\n",
        "\n",
        "        # This is just added to count the number of dents the model thinks it detected\n",
        "        dent_count = len(unique_dent_ids)\n",
        "        count_text = f\"Total Dents Detected: {dent_count}\"\n",
        "        cv2.putText(annotated_frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # frame counter\n",
        "        frame_text = f\"Frame: {frame_count}/{total_frames}\"\n",
        "        cv2.putText(annotated_frame, frame_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "        # update the current state of the process\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Processed {frame_count}/{total_frames} frames, \" f\"Detected {dent_count} unique dents\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Total unique dents detected: {len(unique_dent_ids)}\")\n",
        "    print(f\"Output saved to: {output_path}\")\n",
        "\n",
        "    return len(unique_dent_ids)\n",
        "\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/Assignment3/20251102_102829.mp4\"\n",
        "model_pth = \"/content/drive/MyDrive/Assignment3/best_segmentation_model.pth\"\n",
        "output_path = \"/content/drive/MyDrive/Assignment3/output_tracked_dents_2.mp4\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Loading trained model...\")\n",
        "model = load_trained_model(model_pth, device=device)\n",
        "\n",
        "total_dents = video_processing(video_path, model, output_path, device=device)\n",
        "print(f\"\\n✓ Done! Found {total_dents} unique dents in the video.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
